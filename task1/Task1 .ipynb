{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6th2tFR2AMwo",
        "outputId": "f4648aac-17e6-4aef-f76e-1bc648fcc1b7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rouge_score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge_score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.26.4)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (4.67.1)\n",
            "Building wheels for collected packages: rouge_score\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=c631c986b1e3e9c576efc72e252dcd3f5ca90d0a35347a67b2d69317a1f67118\n",
            "  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
            "Successfully built rouge_score\n",
            "Installing collected packages: rouge_score\n",
            "Successfully installed rouge_score-0.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import pipeline, GPT2LMHeadModel, GPT2Tokenizer\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "# --- Load Dataset ---\n",
        "df = pd.read_csv(\"final_labels.csv\")\n",
        "\n",
        "# Remove rows with missing bodies\n",
        "df = df.dropna(subset=['body'])\n",
        "\n",
        "# Create a directed graph for threading\n",
        "G = nx.DiGraph()\n",
        "for _, row in df.iterrows():\n",
        "    G.add_node(row['entry_id'], body=row['body'], parent=row['parent_id'])\n",
        "    if pd.notna(row['parent_id']):\n",
        "        G.add_edge(row['parent_id'], row['entry_id'])\n",
        "\n",
        "# Function to reconstruct a discussion thread\n",
        "def get_thread(root_id):\n",
        "    thread = []\n",
        "    for node in nx.dfs_preorder_nodes(G, source=root_id):\n",
        "        thread.append(G.nodes[node]['body'])\n",
        "    return \" \".join(thread)\n",
        "\n",
        "# Identify root comments (no parent_id or missing parent)\n",
        "root_comments = df[df['parent_id'].isna()]['entry_id'].tolist()\n",
        "\n",
        "df[\"reconstructed_thread\"] = df[\"entry_id\"].apply(lambda x: get_thread(x) if x in root_comments else None)\n",
        "\n",
        "# --- Summarization ---\n",
        "summarizer = pipeline(\"summarization\", model=\"t5-base\", device=0)\n",
        "\n",
        "def generate_summary(text):\n",
        "    if pd.isna(text) or len(text.split()) < 5:\n",
        "        return text  # Skip short texts\n",
        "    text = \" \".join(text.split()[:512])  # Limit input size\n",
        "    word_count = len(text.split())\n",
        "    max_len = min(150, int(0.75 * word_count))\n",
        "    min_len = min(5, int(0.3 * word_count))\n",
        "    try:\n",
        "        return summarizer(text, max_length=max_len, min_length=min_len, do_sample=False)[0]['summary_text']\n",
        "    except:\n",
        "        return text\n",
        "\n",
        "df[\"summary\"] = df[\"reconstructed_thread\"].apply(generate_summary)\n",
        "\n",
        "df.to_csv(\"thread_summaries.csv\", index=False)\n",
        "\n",
        "# --- Context Mismatch Detection ---\n",
        "similarity_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "def check_context_mismatch(comment, parent_comment):\n",
        "    if not parent_comment:\n",
        "        return \"Missing Parent\"\n",
        "    embeddings = similarity_model.encode([comment, parent_comment], convert_to_tensor=True)\n",
        "    similarity = util.pytorch_cos_sim(embeddings[0], embeddings[1]).item()\n",
        "    return similarity < 0.5  # Mark as mismatch if similarity is low\n",
        "\n",
        "def detect_context_mismatch(row):\n",
        "    if pd.notna(row['parent_id']):\n",
        "        parent_body = df.loc[df['entry_id'] == row['parent_id'], 'body']\n",
        "        if not parent_body.empty:\n",
        "            return check_context_mismatch(row['body'], parent_body.values[0])\n",
        "        else:\n",
        "            return \"Missing Parent\"\n",
        "    return False\n",
        "\n",
        "df['context_mismatch'] = df.apply(detect_context_mismatch, axis=1)\n",
        "\n",
        "df.to_csv(\"thread_summaries_with_mismatch.csv\", index=False)\n",
        "\n",
        "# --- Performance Metrics ---\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "bleu_scores, rouge_scores = [], []\n",
        "\n",
        "for _, row in df.iterrows():\n",
        "    if pd.notna(row['reconstructed_thread']) and pd.notna(row['summary']):\n",
        "        reference = row['reconstructed_thread'].split()\n",
        "        candidate = row['summary'].split()\n",
        "        bleu_scores.append(sentence_bleu([reference], candidate))\n",
        "        rouge = scorer.score(row['reconstructed_thread'], row['summary'])\n",
        "        rouge_scores.append(rouge['rougeL'].fmeasure)\n",
        "\n",
        "# --- Perplexity Calculation ---\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "\n",
        "def calculate_perplexity(text):\n",
        "    if not text or len(text.split()) < 5:  # Avoid very short texts\n",
        "        return np.nan\n",
        "    try:\n",
        "        encodings = tokenizer(text, return_tensors=\"pt\").input_ids.to(model.device)\n",
        "        with torch.no_grad():\n",
        "            loss = model(encodings, labels=encodings).loss.item()\n",
        "        perplexity = torch.exp(torch.tensor(loss)).item()\n",
        "        return perplexity if np.isfinite(perplexity) else np.nan  # Avoid infinite values\n",
        "    except:\n",
        "        return np.nan\n",
        "\n",
        "perplexities = [calculate_perplexity(summary) for summary in df['summary'].dropna()]\n",
        "valid_perplexities = [p for p in perplexities if not np.isnan(p)]\n",
        "\n",
        "# --- Semantic Similarity ---\n",
        "similarities = []\n",
        "for _, row in df.iterrows():\n",
        "    if pd.notna(row['reconstructed_thread']) and pd.notna(row['summary']):\n",
        "        embeddings = similarity_model.encode([row['reconstructed_thread'], row['summary']], convert_to_tensor=True)\n",
        "        similarity = util.pytorch_cos_sim(embeddings[0], embeddings[1]).item()\n",
        "        similarities.append(similarity)\n",
        "\n",
        "# --- Final Results ---\n",
        "avg_bleu = sum(bleu_scores) / len(bleu_scores) if bleu_scores else 0\n",
        "avg_rouge = sum(rouge_scores) / len(rouge_scores) if rouge_scores else 0\n",
        "avg_perplexity = sum(valid_perplexities) / len(valid_perplexities) if valid_perplexities else np.nan\n",
        "avg_similarity = sum(similarities) / len(similarities) if similarities else 0\n",
        "\n",
        "print(f\"Average BLEU Score: {avg_bleu:.4f}\")\n",
        "print(f\"Average ROUGE-L Score: {avg_rouge:.4f}\")\n",
        "print(f\"Average Perplexity: {avg_perplexity:.4f}\")\n",
        "print(f\"Average Semantic Similarity: {avg_similarity:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tpg52b0eEZ21",
        "outputId": "f6641f20-4e6f-4864-e672-867dfe5ba328"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average BLEU Score: 0.1142\n",
            "Average ROUGE-L Score: 0.4294\n",
            "Average Perplexity: 283.0098\n",
            "Average Semantic Similarity: 0.6705\n"
          ]
        }
      ]
    }
  ]
}